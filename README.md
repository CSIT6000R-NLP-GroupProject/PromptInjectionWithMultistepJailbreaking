# Prompt Injection With Multistep Jailbreaking

We apply a Jailbreaking Prompt to attack LLMs
which could effectively mislead the model into out-
putting a malicious text including social biases, re-
inforcing gender, racial, and religious stereotypes.

## Steps
1. Jailbreaking Attacks on Different LLMs


run ```data_process.ipynb```


2. Measure: Label the answers using Detoxify

run ```analysis.ipynb```

3. Result analysis
* Failt to Respond
    * run ```data_analysis.ipynb```
* Mode Comparison
    * run ```result_static.ipynb```
* Model Comparison
    * run ````model_comparison.ipynb```
